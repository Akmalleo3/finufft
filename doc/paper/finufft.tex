\documentclass[10pt]{article}
\textwidth 6.5in
\oddsidemargin=0in
\evensidemargin=0in

\usepackage{graphicx,bm,amssymb,amsmath,amsthm}
\usepackage{showlabels}

% general macros...
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\pig}[2]{\bmp{#1}\includegraphics[width=#1]{#2}\emp}
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\mt}[4]{\left[\begin{array}{rr}#1&#2\\#3&#4\end{array}\right]} % 2x2
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\intR}{\int_{-\infty}^\infty}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\newtheorem{thm}{Theorem}
\newtheorem{cnj}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rmk}[thm]{Remark}
% this work...
\newcommand{\xx}{\mbf{x}}
\newcommand{\sss}{\mbf{s}}
\newcommand{\yy}{\mbf{y}}
\newcommand{\kk}{\mbf{k}}
\newcommand{\KK}{{\mathcal I}}     % Fourier index sets
\newcommand{\freq}{\beta}          % exp sqrt freq param, PSWF c Slepian.
\newcommand{\rat}{\sigma}          % upsampling R ratio, or sigma for Germans
\newcommand{\ppsi}{{\tilde\psi}}   % periodized psi scaled kernel
\newcommand{\rmax}{r_\tbox{dyn}}    % ampl ratio within |k|<N/2, dyn range.
\newcommand{\al}{\alpha}           % dilation
\newcommand{\NU}{{nonuniform}}       % some words I'm fed up of typing...
\newcommand{\U}{{uniform}}
\newcommand{\KB}{Kaiser--Bessel}
\newcommand{\FT}{Fourier transform}



\begin{document}
\title{FINUFFT: A lightweight non-uniform fast Fourier transform library}
\author{Alex Barnett$^{\dag,\ast}$ and Jeremy Magland$^\ast$\\
  $\dag$ Department of Mathematics, Dartmouth College,
    Hanover, NH, 03755 \\
    $\ast$ Flatiron Institute, Simons Foundation,
    New York, NY, 10010
    }
\date{\today}
\maketitle
\begin{abstract}
  Computation of Fourier transforms from data lying at arbitrary
  off-grid locations has many applications ranging from medical
  imaging to astronomy to fast algorithms for PDEs.  We present a
  software library for computing the non-uniform fast Fourier transform
  (NUFFT),
  of types 1 (\NU\ to \U), 2 (\U\ to \NU), and
  3 (\NU\ to \NU), each in dimensions 1, 2, and 3.  Its
  main features are: a simple new kernel allowing faster on-the-fly
  spreading and interpolation from a regular grid, the use of
  quadrature rather than an analytic formula for the kernel Fourier
  transform, multi-threading for efficient use of shared-memory
  machines, simple calling interfaces matching those of the CMCL
  NUFFT library, and bindings to MATLAB/octave and python.
  We analyse the new kernel.
  We
  compare performance of the library to existing CPU-based libraries; typically
  we match the runtime of the NFFT library of Potts et al.\ but
  without the need for a precomputation phase.
\end{abstract}




\section{Introduction}

The computational task addressed
is to evaluate the following exponential sums to a requested precision,
in optimal time.
The type-1 NUFFT (also known as the adjoint NFFT \cite{nfftchap,usingnfft})
in dimension $d$ %=1,2$ or 3
evaluates the Fourier series expansion for a set of
$M$ point sources at arbitrary locations $\xx_j$, which by periodicity
may be taken to lie in $[-\pi,\pi)^d$, and with
strengths $c_j\in\mathbb{C}$,  $j=1,\dots,M$.
The outputs are the Fourier modes with integer indices lying in
the set
$$
\KK = \KK_{N_1,\dots,N_d} := \KK_{N_1} \KK_{N_2} \dots \KK_{N_d}~,
\qquad\mbox{ where } \quad
\KK_{N_i} := \left\{\begin{array}{ll} \{-N_i/2,\ldots,N_i/2-1\}, & N_i \mbox{ even},\\
\{-(N_i-1)/2,\ldots,(N_i-1)/2\}, & N_i \mbox{ odd}.
\end{array}\right.
%\left[-\frac{N_1}{2},\frac{N_1-1}{2}\right] \times \dots \times \left[-\frac{N_d}{2},\frac{N_d-1}{2}\right]
%\label{KK}
$$
In the 1D case $\KK$ is an interval containing $N_1$ integer indices, in 2D it is a rectangle of $N_1N_2$ index pairs, and in 3D it is a cuboid of $N_1N_2N_3$ index triplets.
%\footnote{Note that these counts apply whether each $N_i$ is even or odd.
%  For instance for $N_1=10$ in 1D, the set is $\{-5,-4,\dots,4\}$
%  whereas for $N_1=11$, the set is $\{-5,-4,\dots,5\}$.
%}
We use $N=N_1\dots N_d$ to denote the total number of output values.
We do not address $d>3$ here.
Following the normalization in \cite{dutt,nufft}, then the type-1 NUFFT
evaluates
\be
f_\kk := %\frac{1}{M}   % remove this scaling in code.
\sum_{j=1}^M c_j e^{i \kk\cdot \xx_j}
\qquad \mbox{for } \kk \in \KK_{N_1,\dots,N_d}
\qquad \mbox{(Type-1, or \NU\ to \U\ transform)}
~.
\label{1}
\ee
The naive evaluation of $f$ at all indices $\kk$ requires $\bigO(NM)$
exponential evaluations, which is prohibitive
in many applications.
However, there is a well-known three-step procedure
which computes an approximation with precision $\eps$ with only
$\bigO(M |\log\eps|^d + N \log N)$ work.
Step 1 is to use a smooth kernel of width $w=\bigO(|\log\eps|)$
grid points to resample from $\xx_j$ onto a regular
grid (discrete convolution), step 2 is to
take the discrete Fourier transform on this grid via the usual FFT
(fast Fourier transform), then step 3 is
to divide the resulting amplitudes by the kernel's Fourier transform
(sometimes called ``roll-off correction'').
Thus spreading costs $\bigO(M|\log\eps|^d)$ work, and the other two
steps $\bigO(N \log N)$.
Regardless of $M$, the regular grid is usually chosen to be a constant factor
$\rat$ larger than the desired number of output modes in each dimension.
To minimize errors due to spreading, the kernel should be
well localized in the Fourier domain; since it also must have narrow compact
support, this makes choice of the kernel function critical.
Our library will use this procedure, and the analogous known versions
for the other two types of transforms.

The type-2 transform (or NFFT)
is, up to a normalization factor, the adjoint of the
type-1, and evaluates the Fourier series with given coefficients
$f_\kk$, $\kk\in\KK$, at an arbitrary set of target points
$\xx_j$, $j=1,\ldots,M$, which due to periodicity may be taken to be in $[-\pi,\pi)^d$.
  That is,
  \be
  c_j := \sum_{\kk\in\KK_{N_1,\dots,N_d}} f_\kk e^{i \kk\cdot \xx_j},
  \qquad j=1,\dots, M
\qquad \mbox{(Type-2, or \U\ to \NU\ transform)}
~.
\label{2}
\ee
Finally, the type-3 transform
\cite{nufft3} (or NNFFT \cite{usingnfft})
may be interpreted as evaluating the
Fourier transform of a set of sources with arbitrary locations $\xx_j$
in $\RR^d$
and strengths $c_j$, $j=1,\dots, M$, at the arbitrary target frequencies
$\sss_k$ in $\RR^d$, $k=1,\dots, N$. Note that here $k$ is a plain integer
index.
That is,
\be
f_k := \sum_{j=1}^M c_j e^{i \sss_k \cdot \xx_j}
  \qquad k=1,\dots, N
\qquad \mbox{(Type-3, or \NU\ to \NU\ transform)}
~.
\label{3}
\ee
Note that all three types of transform \eqref{1}, \eqref{2} and \eqref{3}
consist simply of computing an exponential sum.
In certain settings these may be interpretated as quadrature formulae
applied to Fourier transforms.
However, this is not to be confused with the ``inverse NUFFT'' which involves,
for instance, treating \eqref{2} as a large linear system to be solved for
$\{f_\kk\}$ given a right-hand side
$\{c_j\}$ (this is called ``Problem 5'' in \cite{nufft}).
The inverse NUFFT common in Fourier imaging
applications; a popular solution method is to solve
the preconditioned normal equations with
NUFFTs implementing the matrix-vector multiplies
\cite{fessler,fourmont,fastsinc,gelbrecon}.







\subsection{Applications}

MRI - non-Cartesian $k$-space trajectories.
CT.
Spectral interpolation from off-grid data.

Quadrature approximation of Fourier transforms, e.g.\
in image reconstruction \cite{cryo}.
Computation of spatially-periodic solutions to elliptic
PDEs via Ewald summation
\cite{lindbo11}.
Computation of history-dependent part for boundary-integral solvers
for the heat equation.%\cite{strain}.

Fast evaluation of random plane waves cite Beliaev.

extraction of frequencies from astronomy time series.

refs from Fourmont, ultrasound.



\subsection{Existing algorithms and implementations}

Remind that Type 1 standard algorithm is ? :
spreading to U grid using a kernel $\phi(\xx)$.
FFT.
Final correction of the Fourier modes by dividing by
$\hat\phi(k)$.

The first rigorous estimates of the three types of transforms
using the truncated Gaussian kernel in 1D was given by
Dutt--Rokhlin in \cite{dutt}.
The type-3 estimate was improved by Elbel--Steidl \cite{elbel}.


Beylkin and his B-splines. REF.

in 1D, trig poly interp plus FMM for kernel, Dutt--R II.

The CMCL NUFFT package \cite{cmcl} uses truncated Gaussians
and ``fast Gaussian gridding'' \cite[Sec.~3]{nufft}
which reduces the number of exponential evaluations
from $w^d$, where $w$ is the kernel width in grid points,
to $(1+d)w$, giving a claimed 5--10 times acceleration.
On modern architectures RAM access is more of a bottleneck than
flops - DISCUSS.

Cite Nikos+Xiaobai.

It has been observed that the so-called
\KB\ kernel, shown in Fig.~\ref{f:kernel},
\be
\phi_{KB,\freq}(z) := \left\{
\begin{array}{ll}I_0(\freq\sqrt{1-z^2}) / I_0(\freq), & |z|\le 1\\
  0,& \mbox{otherwise}\end{array}\right.
\label{KB}
\ee
where $I_0$ is the regular modified Bessel function of order zero
\cite[(10.25.2)]{dlmf}, has better interpolation
properties than the truncated Gaussian
(CITE Jackson 91 etc).
See Fig.~\ref{f:kernel}(c) which shows the inferiority of the
frequency localization of the optimal truncated Gaussian.
Specifically, for an appropriate choice of $\freq$, it achieves
asymptotically close to twice the exponential localization rate
(see \cite[p.19, (C.1) vs (C.4)]{nfft} which relies on
estimates on the sums of the tails of \eqref{KBhat}
in \cite{fourmontthesis,fourmont,pottshabil}).
In practice this nearly doubles the digits of precision achievable with
a given kernel width.
\eqref{KB} has an analytically known Fourier transform,
\be
\hat\phi_{KB,\freq}(\xi) = \frac{2}{I_0(\freq)}
\frac{\sin \sqrt{\xi^2-\freq^2}}{\sqrt{\xi^2-\freq^2}}
~,
\label{KBhat}
\ee
plotted in Fig.~\ref{f:kernel}(c).
(See \eqref{FT} for our Fourier transform convention.)
Note that in the central region $|\xi|<\freq$,
the square-root is imaginary so that the sine is exponentially large.
The pair \eqref{KB}--\eqref{KBhat}
appears to have been discovered%
\footnote{Curiously, this pair is
  seemlingly absent from standard integral tables \cite[\S 6.677]{GS8}
\cite[\S 2.5.25]{prudnikov1} \cite[\S 2.5.10]{prudnikov2}.}
by B. F. Logan, and its use pioneered by J. F. Kaiser, both at
Bell Labs, in the 1960s \cite{kaiser,kaiserinterview}.

The NFFT package \cite{nfft} from Chemnitz
includes the \KB\ kernel, although it also allows the user to
choose inferior kernels.

Shared-memory parallelization of NFFT \cite{volkmer}.
Automatic tuning based on assumed Fourier coefficient decay \cite{nestler}.
A GPU implementation of the type-2 NUFFT has shown acceleration by around
a factor of 30 relative to NFFT in 1D and 2D \cite{cunfft}.

Fessler and Sutton \cite{fessler} use optimization in the space of
interpolation weights, enabling a slight increase of around 1/3 of a
digit in accuracy over \KB\ in 1D. However, they conclude that
``the
Kaiser--Bessel interpolator, with suitably optimized parameters,
represents a very reasonable compromise between accuracy and
simplicity.''

Recently Ruiz-Antol\'in and Townsend
\cite{townsendnufft} devised an algorithm
based on stable low rank approximation, that
trades upsampling for the evaluation of several FFTs (of order $w^d$ of them);
this may have advantages in certain parallel settings.



\bfi[t]  % fffffffffffffffffffffffffffffffffffffff
\hspace{-2ex}\ig{width=6.7in}{kernel.eps}
\ca{The proposed ES spreading kernel
  \eqref{ES} (solid blue lines), compared to the \KB\ \eqref{KB}
  (dashed green lines) and truncated Gaussian (dotted pink lines) kernels.
  We show the asymptotically optimal
  Gaussian kernel $\phi(z) = e^{-0.46 \beta z^2}$ in $|z|\le 1$, zero otherwise.
  (a) shows the kernels for $\freq=4$; the discontinuities at
  $\pm 1$ are highlighted by dots.
  (b) shows a logarithmic plot (for positive $z$) of the
  kernels for $\freq=30$
  (corresponding to a spreading width of 13 grid points).
  The graph for ES is a quarter-circle.
  (c) shows a logarithmic plot of the magnitude of the
  Fourier transform of the kernels.
  Note for ES and KB the quarter-circular shape in $|\xi|<\freq$.
  All three have exponentially small values uniformly in $|\xi|>\freq$, but
  for the Gaussian the exponential convergence rate is only around half
  the rate for the other two kernels.
}{f:kernel}
\efi


\subsection{Contribution of this work}

We describe a software implementation of the NUFFT
which exploits a new kernel whose Fourier localization 
is essentially equal to that of \eqref{KB},
but which is faster to evaluate numerically.
This results in improved numerical performance and simpler code and interfaces.
In 1D, in rescaled spatial units, this kernel is
\be
\phi(z) = \phi_\freq(z) :=
\left\{\begin{array}{ll}
e^{\freq (\sqrt{1-z^2}-1)}, & |z|\le 1\\
0, & \mbox{otherwise}
\end{array}
\right.
\label{ES}
\ee
%where $\freq>0$ is a width parameter that must be set
%in accordance with the kernel width measured in uniform grid point units.
which we call the ``exponential square-root'' (ES) kernel.
In higher dimensions we use products of this 1D kernel.

Fig.~\ref{f:kernel} suggests that as $\freq\to\infty$ the ES kernel
becomes very similar to \KB.
Indeed, note the asymptotic form
$I_0(x) \sim \frac{e^x}{\sqrt{2\pi x}}$ \cite[(10.30.4)]{dlmf}.
Dropping the denominator, i.e.\ replacing $I_0(x)$ by $e^x$ in \eqref{KB},
and normalizing so that $\phi(0)=1$, gives \eqref{ES}.
No analytic Fourier transform of \eqref{ES} is known,
but in thm ** below we prove asymptotic
bounds that are very simliar to those known
from \eqref{KBhat}.
GIVE EXP RATE?
Although we cannot prove a rigorous estimate on the sum of the tail,
we do bound the tail uniformly.
This provides confidence in the numerical use of the kernel.
Remarkably, the Bessel function $I_0(x)$ in \eqref{KB}
appears not to be crucial for excellent Fourier localization.


The features of our software implementation include:
\bi
\item use of the ES kernel with accuracy very similar to \KB\ but
  faster evaluation.
\item use of quadrature rather than an analytic formula to evaluate
  the kernel Fourier transform needed for the roll-off correction
  (deconvolution) phase.
\item parallel spreading and deconvolution on shared-memory machines via OpenMP.
\item use of the multi-threaded FFTW3 library for FFTs.
\item compilation options such as single-precision (to reduce RAM footprint)
  and/or single-threaded.
\item bindings to MATLAB, octave, and python.
\ei

%Unlike the authors of some other libraries,
We take the philosophy that the user calls the library to approximate the
exponential sums \eqref{1}--\eqref{3} to the requested precision.
Thus, the user does not have direct control over the type and width of
the spreading kernel; indeed, it would be confusing to have such control.
Rather, once the requested precision is given, such decisions
are made ``under the hood'' by the library.

We do performance tests blah.
Summary of rest of paper.

In section \ref{s:err} we review the error analysis of the type-1 and type-2
NUFFT and show ***
Sec.~\ref{s:optim} sheds light on the somewhat mysterious
connection between the ES and KB kernels and prolate spheroidal wavefunctions.




\section{Use of the library}

The interface to the library is as
%straightforward
simple as possible.
From C++, with {\tt x} a {\tt double} array of {\tt M} source points,
{\tt c} a complex ({\tt std::complex<double>}) array of {\tt M} strengths,
and {\tt N} an integer number of desired output modes,
\begin{verbatim}
finufft1d1(M,x,c,isign,acc,N,f,opts);
\end{verbatim}
computes the 1D type-1 NUFFT to precision {\tt acc}, writing the
answer into the complex array {\tt f} preallocated by the user.
Setting {\tt isign} either $1$ or $-1$ controls the
sign of the imaginary unit in \eqref{1}.
{\tt opts} is a structure with fields controlling various options;
for example setting {\tt opts.debug=1} prints internal timing breakdowns.
The function returns an integer zero if successful, otherwise
its value indicates the type of error found.
We emphasize that there is no ``plan'' stage
(although this may be part of a future release).
This makes the library extremely simple to use.
The other eight routines have analogous interfaces.


Demo calling from C, Fortran, Matlab/octave, python.
ETC


\section{Algorithms}

Our implementations mostly follow known procedures
for evaluating the type-1, 2 and 3 NUFFTs.
However, since we introduce novelties such as the
use of a kernel without analtyically known Fourier transform,
here we give a complete description.

We use the Fourier transform convention
\be
\hat\phi(k) = \intR \phi(x) e^{ikx} dx
~,\qquad
\phi(x) = \frac{1}{2\pi} \intR \hat\phi(k) e^{-ikx} dx
~.
\label{FT}
\ee
% means ``causal'' (in x) is analytic in UHP of k, FWIW.
% and k>0 decays in complex x UHP.


\bfi[t]  % fffffffffffffffffffffffffffffffffffffff
\ig{width=6.5in}{spreadalias.eps}
\ca{(a) 1D illustration of spreading from \NU\ points to the grid
  values $b_l$, $l=0,\dots,n-1$ (shown as dots) needed for type 1.
  For clarity, only two \NU\ points $x_1$ and $x_2$ are shown;
  the former results in periodic wrapping of the effect of the kernel.
  The continuous kernel function contributions are shown in pink.
  (b) Semi-logarithmic plot of the (positive half of the)
  Fourier transform of the rescaled
  kernel $\psi(x)$, showing the usable frequency domain (and the
  dynamic range $\rmax$ over this domain), and approximate
  but useful relations between the precision $\eps$
  and the kernel frequency parameter $\beta$.
  These are well approximated by assuming the shape of the curve is
  a quarter circle or ellipse.
}{f:spreadalias}
\efi




% 11111111111111111111111111111111111111111111111111111111111111111111111
\subsection{Type 1: \NU\ to \U}

We describe the algorithm to compute $\tilde f_\kk$, an approximation
to $f_\kk$ defined by \eqref{1}.
We have fixed an upsampling ratio $\rat>1$ which will control the
upsampled grid size.
Following many researchers CITE we find $\rat=2.0$ adequate;
increasing $\rat$ can reduce $w$ slightly for the same precision,
but increases the RAM and FFT effort.
*** REF on tuning.
Then, given a requested precision $\eps$,
an integer kernel width $w$ is chosen
to be the smallest integer at least $|\log_{10} \eps| + 1$,
and the kernel frequency parameter $\freq = 2.3 w$.
These rules of thumb, appropriate for the ES or KB kernel with
$\rat=2$, will be justified in Sec.~\ref{s:w}.

\subsubsection{1D case}
\label{s:1d1}

For simplicity we start with the 1D case, using $x_j$ to denote source
locations and $k\in\KK$ to label the $N=N_1$ output modes.
The DFT size will be $n \approx \rat N$,
but, for convenience in the spreading code, we insure that $n$
is not less than $2w$,
and for efficiency of the FFT we also increase $n$ until it reaches the next
integer of the form $2^q3^p5^r$.

{\bf Step 1 (spreading).}
The kernel $\phi$ \eqref{ES} has support $[-1,1]$, but we wish to
use a rescaled version with support $[-\al,\al]$, where
the dilation factor is the desired kernel half-width
\be
\alpha := wh/2 = \pi w/n
~,
\label{al}
\ee
where $w$ is the kernel full width in grid points, $n$
the upsampled grid size, and $h := 2\pi/n$ the upsampled grid spacing.
For the rescaled kernel we write
\be
\psi(x) := \phi(x/\al)~,
\qquad \hat\psi(k) = \al \hat\phi(\al k)~,
\qquad \mbox{(1D case)}
\label{psi1}
\ee
and for its periodization,
\be
\ppsi(x) := \sum_{m\in\ZZ} \psi(x-2\pi m)
~.
\qquad \mbox{(1D case)}
\label{ppsi1}
\ee
We then compute, at a cost of $wM$ kernel evaluations, % and $\bigO(wM)$ flops,
the discrete convolution
\be
b_l = \sum_{j=1}^M c_j \ppsi(lh - x_j)
~, \qquad \mbox{for } l=0,\dots,n-1
~,
\label{bl1}
\ee
as shown in Fig.~\ref{f:spreadalias}(a).
Because of periodicity, here the $l$ index is defined only up to modulo $n$.

{\bf Step 2 (FFT).}
We use the FFT to evalute the $n$-point DFT
\be
\hat{b}_k = \sum_{l=0}^{n-1} e^{2\pi i lk/n} b_l ~, \qquad \mbox{ for } k\in\KK_n
~.
\label{dft1}
\ee
Note that the output indices $k$ are cyclically equivalent to the
set $k=0,\dots,n-1$ that is the usual ordering for the FFT.

{\bf Step 3 (correction).}
We truncate (to the central $N$ frequencies) and
diagonally scale the amplitudes array, to give
the approximant to $f_j$, namely
\be
\tilde f_k = p_k \hat{b}_k ~, \qquad \mbox{ for } k\in\KK
~,
\label{pb1}
\ee
where an optimal choice of the correction factors $p_k$ comes from
samples of the Fourier transform%
\footnote{It is tempting instead to set $p_k$ to be the {\em discrete} FT
  of the grid samples of the kernel $\{\ppsi(lh)\}_{l=0}^{n-1}$.
  However, in our
  experience this causes around twice the error of \eqref{pk1},
  as can be justified by the discussion in Sec.~\ref{s:err}.}
of the scaled kernel,
\be p_k = h / \hat\psi(k)~ = 2/(w\hat\phi(\alpha k)), \qquad k\in\KK
\label{pk1}
~.
\ee
%The choice \eqref{pk} will be justified to produce the desired accuracy
%in Sec.~\ref{s:err} below.
In contrast with existing NUFFT algorithms
which rely on knowing an analytic formula for $\hat\psi(k)$,
we approximate $\hat\psi(k)$
numerically. For this we use $2p$-node Gauss--Legendre quadrature
on the Fourier integral. Let $q_j$ and $w_j$ be the nodes and weights
for $2p$-node quadrature on $[-1,1]$.
By exploiting the reality and even symmetry of the kernel,
only the $p$ positive nodes are needed, thus,
$$
\hat\psi(k) \;=\; 
\int_{-\al}^{\al} \psi(x) e^{ikx} dx
\;\approx\;
wh \sum_{j=1}^p w_j \phi(q_j) \cos (\al k q_j)
~.
$$
In practice we find that $p\ge 2+1.5 w$ gives sufficient accuracy
over the needed range $|k|\le N/2$,
%apparent exponential convergence rate 
meaning that the maximum quadrature spacing is around one
grid spacing $h$.
We discuss this choice further in Sec.~\ref{s:p}.
The cost of the evaluation of $p_k$ is $\bigO(pN)$,
and naively would involve $pN$ cosines.
By exploiting the fact that, for each quadrature point $q_j$,
successive values of $e^{i \al k q_j}$ over the regular $k$ grid are
related by a constant phase factor, these cosines
can be replaced by $p$ complex exponentials and $pN$ adds and multiplies.
We call this standard addition-formula trick
``phase winding.''\footnote{In the code, see the function
  {\tt onedim\_fseries\_kernel} in {\tt src/common.cpp}}
  


\subsubsection{The case of higher dimensions $d>1$}

For 2D or 3D, in general different
upsampled grid sizes are needed in each dimension,
chosen by the same recipe, so that $n_i \ge \rat N_i$, $n_1 \ge 2w$,
$n_i = 2^q3^p5^r$, $i=1,\dots,d$.
For the kernel we use products of the 1D kernel scaled appropriately
in each dimension,
\be
\psi(\xx) = \phi(x_1/\al_1) \cdots \phi(x_d/\al_d)
~,
\ee
where $\al_i=\pi w/n_i$.
The periodic kernel \eqref{ppsi1} is then
\be
\ppsi(\xx) := \sum_{\mbf{m} \in \ZZ^d} \psi(\xx - 2\pi\mbf{m})
~.
\label{ppsi}
\ee
With $h_i:=2\pi/n_i$ denoting the grid spacing in each dimension,
and $\mbf{l}:=(l_1,\dots,l_d)$ the index,
the discrete convolution %\eqref{bl1}
becomes
\be
b_\mbf{l} = \sum_{j=1}^M c_j \ppsi((l_1h_1,\dots,l_dh_d)-\xx_j)~,
\qquad l_i=0,\dots,n_i-1, \quad i=1,\dots,d
~.
\label{bl}
\ee
In evaluating \eqref{bl}, separability
means that only $wd$ kernel evaluations are needed per source point:
the $w^d$ square or cube of $\ppsi$ values is then filled as a tensor product.
The correction factor is then also separable,
\be
p_\kk = h_1\dots h_d \hat\psi(\kk)^{-1} = (2/w)^{d}
(\hat\phi(\al_1 k_1) \cdots \hat\phi(\al_d k_d))^{-1}
~, \qquad \kk \in \KK~,
\label{pk}
\ee
so that only $d$ 1D Fourier transforms of $\phi$ need be evaluated.
Each such Fourier transform is output on a regular grid, so the phase winding
trick is used.
The DFT \eqref{dft1} generalizes in the standard way to
multiple dimensions.



% 222222222222222222222222222222222222222222222222222222222222222222222222222
\subsection{Type 2: \U\ to \NU}
\label{s:2}

From now on we present formulae in general dimension $d$.
To compute $\tilde c_j$, an approximation to $c_j$ in \eqref{2},
we reverse the steps for the type-1.
Given the number of modes $N$, and the precision $\eps$,
the choices of $n$, $w$ and $\beta$ are as in the type-1.

{\bf Step 1 (correction).}
The input coefficients $f_\kk$ are pre-corrected and zero-padded,
\be
\hat b_\kk = \left\{\begin{array}{ll}p_\kk f_\kk~, & \kk \in \KK \\
0~, & \kk \in \KK_{n_1,\dots,n_d} \backslash \KK
\end{array}\right.
\ee
with amplification factors $p_\kk$ as in \eqref{pk} computed by phase winding.

{\bf Step 2 (FFT).}
This is just as in type-1. Writing the general dimension
case of \eqref{dft1}, with the index vectors $\mbf{l}$
and $\kk$ (and their ranges) swapped,
\be
b_\mbf{l} = \sum_{\kk\in\KK_{n_1,\dots,n_d}}
e^{2\pi i (l_1k_1/n_1 + \dots + l_dk_d/n_d)}
\,\hat b_\kk ~, \qquad \mbox{ for }
\qquad l_i=0,\dots,n_i-1, \quad i=1,\dots,d
~.
\label{dft}
\ee

{\bf Step 3 (interpolation).}
The adjoint of spreading is interpolation, which
outputs a weighted admixture of the
grid values near to each target point.
The approximant to the answer $c_j$ is thus
\be
\tilde c_j = \sum_{l_1=0}^{n_1-1} \cdots \sum_{l_d=0}^{n_d-1}
b_\mbf{l} \ppsi((l_1h_1,\dots,l_dh_d) - \xx_j)
~.
\label{interp}
\ee
As with the type-1,
because of separability,
this requires $wd$ evaluations of the kernel function per target.



% 33333333333333333333333333333333333333333333333333333333333333333333333333
\subsection{Type 3: \NU\ to \NU}

Recall that in the type 3 transform,
both the source and target values are generally \NU.
Our algorithm is standard, being
the general kernel version of the truncated Gaussian algorithms in
\cite[Alg.~3]{nufft} \cite[Alg.~2]{elbel} \cite{nufft3}
\cite[Sec.~1.3]{nfftchap}.
Loosely speaking, the algorithm is a
type-1 ``wrapped around'' a type-2, %specifically
where the type-2 replaces the middle FFT step from the type-1.

Given $\eps$, we choose $w$ and $\freq$ as in the type-1.
However, the upsampled array size $n_i$ in each dimension
will instead be proportional to the product of the following bounds on the
source and target coordinates,
in each dimension $i=1,\dots,d$:
\be
X_i := \max_{j=1,\dots,M} |x_j^{(i)}|
~,\qquad
\xx_j = (x_j^{(i)},\dots,x_j^{(d)})
~,\qquad\mbox{ and }
\quad
S_i := \max_{k=1,\dots,N} |s_k^{(i)}|
~,\qquad
\sss_k = (s_k^{(i)},\dots,s_k^{(d)})
~.
\label{XS}
\ee
The precise choice of $n_i$ is most easily understood after the
steps of the algorithm are given.

{\bf Step 1 (dilation and spreading).}
For spreading onto a grid on $[-\pi,\pi)^d$,
a dilation factor $\gamma_i$ needs to be chosen
for each dimension $i=1,\ldots,d$ 
such that the rescaled sources ${x'_j}^{(i)} := x_j^{(i)}/\gamma_i$
lie in $[-\pi,\pi]$. Furthermore these sources must be
at least $w/2$ grid points
from the ends $\pm\pi$ in order
to avoid wrap-around of mode amplitudes in Step 2.
%The latter condition is needed since the input mode indices for the type-2 are not periodic.
This is expressed by
\be
X_i/\gamma_i \le \pi(1 - w/n_i)
~, \qquad i=1,\dots,d
\label{cond1}
\ee
We may then rewrite \eqref{3} as
$f_k := \sum_{j=1}^M c_j e^{i \sss'_k \cdot \xx'_j}$, $k=1,\dots, N$,
where ${s'_k}^{(i)} = \gamma_i s_k^{(i)}$.

With $\xx'_j$ defined as above,
we spread onto a regular grid using the usual
periodized kernel \eqref{ppsi}, to get
\be
\hat b_\mbf{l} = \sum_{j=1}^M c_j \ppsi((l_1h_1,\dots,l_dh_d)-\xx'_j)~,
\qquad \mbf{l} \in \KK_{n_1,\dots,n_d}~.
\label{blt3}
\ee
%(in constrast to for the type-1 and type-2),
Unlike before, we have chosen a (cyclically equivalent) output index
grid centered at the origin, because we
shall now interpret $\mbf{l}$ as a Fourier mode index.

{\bf Step 2 (Fourier series evaluation via type-2 NUFFT).}
Treating \eqref{blt3} as a set of Fourier series coefficients, we
evaluate this series at rescaled target points, thus,
\be
b_k = \sum_{\mbf{l} \in \KK_{n_1,\dots,n_d}}
\!\! \hat b_\mbf{l} \, e^{i\mbf{l}\cdot \sss''_k}
\,\qquad k=1,\dots,N
~,
\label{bkt3}
\ee
where the rescaled frequency targets have coordinates
${s_k''}^{(i)} := h_i {s_k'}^{(i)} = h_i\gamma_i s_k^{(i)}$, $i=1,\dots,d$.
Here the new factor $h_i$ arises because the spatial grid of spacing $h_i$
has to be stretched to unit spacing to be interpreted as a Fourier series.
The type-2 NUFFT (see Sec.~\ref{s:2}) is used to evaluate \eqref{bkt3}.

{\bf Step 3 (correction).}
Similarly to the final step of the type-1,
in order to compensate for the spreading of step 1 (in primed coordinates)
a diagonal correction is applied,
$$
\tilde f_k = p_k b_k
~,\qquad 
\mbox{ where }
\quad p_k = h_1\dots h_d \hat\psi(\sss'_k)^{-1} = (2/w)^d
\bigl(
\hat\phi(\alpha_1{s'_k}^{(1)}) \cdots \hat\phi(\alpha_d{s'_k}^{(d)})
\bigr)^{-1}
~,
\qquad k=1,\dots,N
~.
$$
But, in contrast to the case of types 1 and 2,
the set of frequencies at which $\hat\phi$ must be evaluated is %in general
\NU, so there is no way to exploit the phase winding trick.
Rather, $dpN$ cosines must be evaluated,
recalling that $p$ is the the number of positive quadrature nodes.
Despite this cost, this step consumes only a small fraction of the
total computation time.

\begin{rmk}
  We have found that using the same overall requested precision $\eps$ to
  choose the parameters for steps 1 and 2 of this algorithm
  gives overall error close to $\eps$.
  The earliest type-3 error bounds for the Gaussian kernel are only twice that
  for the type-1 or type-2 \cite[Obs.~5.6]{nufft}.
  However, sharper bounds \cite[p.~45]{elbel} for all three types produce
  bounds for the type-3 where the error from step 2 is multiplied by
  $\rmax$, defined by \eqref{rmax}.
  In our setting $\rmax \le 10$, which may explain our empirical finding
  that no reduction in the $\eps$ for step 2 is needed.
\end{rmk}
% *** check why don't need an extra digit in t-2 due to rmax factor.


{\bf Recipe for parameter choice (Step 0).}
Finally we are ready to give the recipe for choosing the upsampled grid
sizes $n_i$, which of course in practice precedes the above three steps.
This relies on aliasing error estimates \cite{elbel}
for steps 1 and 3 that we explain here only heuristically.
%(step 2 is treated as a black box).
In Sec.~\ref{s:err} we will see that spreading onto a uniform grid
of size $h_i$ induces a lattice of
aliasing images separated by $n_i$ in frequency space,
so that the correction step is only accurate to precision $\eps$ out to
frequency magnitude $n_i/2\rat$.
Thus, since $|{\sss'_k}^{(i)}| \le \gamma_i S_i$ for all $i$ and $k$,
the condition
\be
\gamma_i S_i \le \frac{n_i}{2\rat}
~,\qquad i=1,\dots,d
\label{cond2}
\ee
is sufficient.
Combining \eqref{cond1} and \eqref{cond2} and solving as equalities
gives the recipe for the optimal parameters (similar to \cite[Rmk.~1]{nufft3}),
\be
n_i = \frac{2\rat}{\pi}X_iS_i + w
~,\qquad
\gamma_i = %\max\bigl[
\frac{X_i}{\pi(1 - w/n_i)}
%\frac{1}{S_i} \bigr]
~.
\qquad i=1,\dots,d
\label{ng}
\ee
\begin{rmk}[size of grid required]
  The product of the grid sizes $n_i$ in each dimension $i=1,\dots,d$
  sets the number of modes, hence the FFT effort required,
  in the type-2 transform in step 2.
Crucially,  this is independent of the numbers of sources $M$ and of
  targets $N$.
  Rather, $n_i$ scales like the space-frequency product $X_iS_i$.
  This connects to the Fourier uncertainty principle:
  $n_i$ scales as the number of ``Heisenberg boxes''
  %\cite{mallatbook}
  needed to fill the centered rectangle enclosing the data.
  In fact, since the number of degrees of freedom \cite[p.~391]{slepianrev}
  (or ``semiclassical basis size'' \cite{davisheller})
  needed to represent functions
  living in the rectangle $[-X_i,X_i]\times[-S_i,S_i]$ is its area divided
  by $2\pi$, namely $2X_iS_i/\pi$, we see that
  $n_i$ is asymptotically $\rat$ times this basis size.
  Thus the total number of grid points $n_1\dots n_d$ is $(\rat/2\pi)^d$
  times the volume of the smallest centered $2d$-dimensional space-frequency
  cuboid enclosing all of the data $\xx_j$, $\sss_k$.
  \label{r:heis}
\end{rmk}

{\bf Efficiently handling poorly-centered data.}
When the smallest interval containing any coordinate of the input data
$\xx_j$ or $\sss_k$
is not well centered on the origin, the bound $X_i$ or $S_i$ given
by \eqref{XS}, hence $n_i$ and the computational cost,
will be unnecessarily large.
Translations in $x$ or $s$ are cheap to apply,
as can be seen by factoring
\be
\sum_{j=1}^M c_j e^{i (\sss_k+\sss_0) \cdot (\xx_j+\xx_0)}
\;= \;
e^{i(\sss_k+\sss_0)\cdot\xx_0} \sum_{j=1}^M (e^{i\sss_0\cdot \xx_j}c_j) e^{i \sss_k\cdot\xx_j}~.
\label{trans}
\ee
This shows that type-3 transform with translated data
can be applied by pre-phasing the strengths by $e^{i\sss_0\cdot \xx_j}$,
doing the transform, then post-multiplying by $e^{i(\sss_k+\sss_0)\cdot\xx_0}$.
The extra cost is $\bigO(N+M)$ complex exponentials.
In our implementation we
choose the optimal translations, namely the mean of the largest and
smallest coordinate in space, and in frequency, for each dimension.

\begin{rmk}[type-3 failure mode]
  %The max is to stop failure when X=0
%  Despite optimal translation of the input data,
  Remark~\ref{r:heis} shows that input data can be chosen for which the
  algorithm is arbitrarily inefficient.
  For example, with only two points ($M=N=2$) in 1D with
  $x_1=-X$, $x_2=X$, $s_1=-S$, $s_2=S$,
  then by choosing $XS$ huge, \eqref{ng} implies that the algorithm
  will require a huge amount of memory and time.
  Obviously in this case a direct summation of \eqref{3} is preferable.
  However, for $N$ and $M$ large but with highly clustered data,
  a butterfly-type algorithm which hierarchically exploits \eqref{trans}
  could be designed. Since we cannot assume such clustering in most
  applications, we do not address this further.
  %We do not cover this specialized case here.
\end{rmk}




\section{Error analysis}

For convenience we first derive a known formula
for the aliasing error in the 1D type 1 and 2 NUFFT
in exact arithmetic.
This error usually dominates over rounding error
(see \cite[\S 1.4]{nfftchap} for rounding error analysis).
In later subsections we present bounds on the \FT\ of the ES kernel,
and use them to motivate the algorithm parameter choices from the
previous section.
The section concludes with a discussion of the optimality of,
and connections between, the KB and ES kernels, and prolate spheroidal
wavefunctions.

\subsection{Error in the type-1 and type-2 transforms with general kernel}
\label{s:err}

We first need the
Poisson summation formula
\cite{apostol} generalized to include a phase $e^{i\theta}$:
for any continuous function
$\psi \in L_1(\RR)$ with Fourier transform $\hat\psi$,
and any lattice spacing $h>0$,
\be
\sum_{l\in\ZZ} e^{il\theta} \psi(x - lh) \; = \;
\frac{1}{h} \sum_{m\in\ZZ}
\hat\psi\biggl(-\frac{2\pi m + \theta}{h}\biggr)
\exp \biggl({i\,\frac{2\pi m + \theta}{h}x}\biggr)
~.
\label{pois}
\ee
This can be proven, as usual, by noticing that multiplication
by $e^{-ix\theta/h}$ makes the left-hand side periodic,
then writing the Euler--Fourier formula for the coefficients of its
Fourier series.

We now derive the error using a general rescaled kernel $\psi(x)$.
For the type-1, inserting \eqref{bl1} into \eqref{dft1}
and the result into \eqref{pb1}, and subtracting from the true answer
\eqref{1} gives the error
$$
\tilde f_k - f_k  \;=\;
\sum_{j=1}^M c_j \left[ p_k \sum_{l=0}^{n-1} e^{2\pi i l k/n}
  \ppsi(lh-x_j) - e^{ikx_j} \right ]
\;=:\; \sum_{j=1}^M c_j g_k(x_j)
~, \qquad k\in\KK,
$$
where, applying Poisson summation \eqref{pois} with $\theta=hk$,
%and symmetry of $\hat\psi$,
$$
g_k(x) :=  p_k \sum_{l=0}^{n-1} e^{2\pi i l k/n}
\ppsi(lh-x) - e^{ikx}
= p_k \sum_{l\in\ZZ} e^{ilhk} \psi(lh-x) - e^{ikx}
=
\frac{p_k}{h} \sum_{m\in\ZZ} \hat\psi(k+mn) e^{i(k+mn)x} - e^{ikx}
~.
$$
This motivates the choice of $p_k$ in \eqref{pk1} which
kills exactly the contributions from
$m=0$, leaving the known aliasing error formula \cite[(1.16)]{nfftchap}
\cite[(4.1)]{fourmont} \cite[Sec.~V.B]{fessler}
\be
g_k(x) = \frac{1}{\hat\psi(k)}\sum_{m\neq 0} \hat\psi(k+mn) e^{i(k+mn)x}
~.
\label{gkx}
\ee
Since $|k|\le N/2$,
error is thus controlled by the phased sum of the tails of $\hat\psi$
at frequency magnitudes at least $n-N/2$. See Fig.~\ref{f:spreadalias}(b).
%Other choices of $p_k$ leave some contribution from $m=0$, which usually
%dominate.
% footnote re why not choose DFT of kernel?

Since type-2 is the adjoint of type-1 (or by similar manipulations to the
above), its error is
\be
\tilde c_j - c_j = \sum_{k\in\KK} f_k g_k(x_j)
~, \qquad j=1,\dots,M~.
\ee
A common way to summarize errors for both transforms is then
to assume a uniform bound $\eps$ on $g_k$, so
\be
\max_{k\in\KK}|\tilde f_k - f_k| \le \eps \|\mbf{c}\|_1
\quad \mbox{ (type-1)~, } \qquad
\max_{1\le j\le M}|\tilde c_j - c_j| \le \eps \|\mbf{f}\|_1
\quad \mbox{ (type-2)~, }
\label{1nrm}
\ee
if $|g_k(x)| \le \eps$, for all $|k|\le N/2$, $x\in\RR$,
and $\mbf{c}$ and $\mbf{f}$ are the respective vectors of input data.
We can separate the variation in the numerator and denominator
of \eqref{gkx} to get the estimate
\be
\eps := \max_{|k|\le N/2}
\|g_k\|_\infty \le
\frac{
  \max_{|k|\le N/2} \left|
    \sum_{m\neq 0} \hat\psi(k+mn) e^{i(k+mn)x}
  \right|
}{\min_{|k|\le N/2} |\hat\psi(k)|}
~.
\label{epsest}
\ee
Since the dynamic range of the denominator is small, and the tails
of the sum decay slowly, we believe that this estimate is close to tight.

In the next section will insert estimates on $\hat\psi$ into
\eqref{epsest}, to get rigorous bounds for the ES kernel.
For now, a non-rigorous but useful picture of the error size
is sketched in Fig.~\ref{f:spreadalias}(b):
assuming that i) $\hat\psi(k)$ decreases monotonically
with $|k|$ for $|k|\le N/2$, and  ii) the worst-case sum (numerator in \eqref{epsest}) is dominated by the single value with smallest $|k|$,
then we get $ \eps \approx |\hat\psi(n-N/2) / \hat\psi(N/2) |$,
whose logarithm is shown in the figure.

\begin{rmk}
  If the kernel $\psi$ is discontinuous, for instance at $\pm1$,
  this implies that the tail of $\hat\psi$
  is a sinc function dying no faster than $1/k$, which is conditionally
  convergent.
  Then estimates of \eqref{epsest}
  which discard the phase and sum the absolute value of each term
  are not finite. 
%$$
%\eps \le
%\frac{\max_{|k|\le N/2} \sum_{m\neq 0} |\hat\psi(k+mn)|}{\min_{|k|\le N/2} |\hat\psi(k)|}~,
%$$
For such class of kernel (which includes KB and our ES),
more subtle estimates which retain phase information in the
conditionally-convergent sum \eqref{epsest} are needed
(see remark~\ref{fourmont} below).
For the truncated Gaussian kernel, error estimates handle
the truncation in other ways \cite{nufft,elbel,nfftchap}.
\end{rmk}


% ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
\subsection{Asymptotic estimates on the Fourier transform of the ES kernel}

Recall the relation \eqref{psi1} stating that $\psi(x)$ is a rescaled
version of $\phi(z)$, the kernel defined on $[-1,1]$.
For simplicity we work with $\phi$.
To present asymptotics in the width parameter $\freq\to\infty$,
it is convenient to express the Fourier transform with the scaled frequency
\be
\rho \;:=\; \xi/\freq ~.
\label{rho}
\ee
We will fix $\rho$, and let the frequency $\xi=\rho\freq$ grow in proportion
to $\freq$. The cutoff frequency (see vertical line
in Fig.~\ref{f:kernel}(c)) is then at $\rho=1$.
The following shows that, up to weak algebraic prefactors,
(a) below the cutoff $\hat\phi$ has the same form of as $\phi$,
and that (b) above the cutoff $\hat\phi$ is uniformly exponentially small,
with the same exponential rate $e^\beta$ as occurs for the \KB\ kernel.

\bfi[t] % fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
\pig{2.2in}{saddlea_lab}
\pig{2.2in}{saddleb_lab}
\pig{2.2in}{saddleft}
\ca{
  Real part of the integrand $e^{\freq p(z)}$
  appearing in the ES kernel \FT\
  (see \eqref{ESFT}),
  plotted across the complex $z$ plane, for $\freq=30$.
  (a) $\rho=0.8$, below cutoff frequency.
  Also shown are saddle point $z_0$ ($\ast$), example contour (line with arrow),
  boundaries where $\re p(z) = \re p(z_0)$ (dotted lines),
  and standard branch cuts (wiggly lines).
  Inset shows elliptic coordinate plane $(u,v)$ for the right half of the
  $z$-plane, and ``bad'' region where $\re p(z) > \re p(z_0)$ (shaded).
  (b) $\rho =1.2$, above cutoff.
  %frequency in the exponentially small aliasing error tail.
  Note the change in color scale. The branch cuts of the square-root have
  been rotated to point downwards, exposing the two saddle points.
  (c) Comparison of the asymptotic formulae \eqref{EShat1}--\eqref{EShat2}
  from Thm.~\ref{t:EShat}
  to the true $\hat\phi$ (evaluated accurately by quadrature).
  The weak algebraic divergence at $\rho=1$ has been highlighted by plotting
  a vertical asymptote.
}{f:saddle}
\efi


\begin{thm} % -----------------------------------------------------------------
  Let $\hat\phi$ be the Fourier transform (as in \eqref{FT}) of the ES kernel
  $\phi$ given by \eqref{ES}.
  
  (a)
  Fix $\rho\in(-1,1)$, ie, below cutoff. Then, %the asymptotic result holds,
\be
\hat\phi(\rho\freq) \; =\;
\sqrt{\frac{2\pi}{\beta}} \frac{1}{(1-\rho^2)^{3/4}} e^{\freq(\sqrt{1-\rho^2}-1)}
\left[ 1 + \bigO(\freq^{-1}) \right]
~, \qquad \freq\to\infty
\label{EShat1}
\ee

(b)
  Fix $\rho$, $|\rho| > 1$, ie, above cutoff. Then,
\be
\hat\phi(\rho\freq) \; =\;
2\sqrt{\frac{2\pi}{\beta}}
e^{-\freq}
\frac{\sin\left(\freq\sqrt{\rho^2-1} - \pi/4\right)}
{(\rho^2-1)^{3/4}}
\left[ 1 + \bigO(\freq^{-1}) \right]
~, \qquad \freq\to\infty
\label{EShat2}
\ee
%sqrt(2*pi/beta)*exp(-beta)*2*sin(-pi/4+beta*sqrt(rt.^2-1)).*(rt.^2-1).^(-.75);
\label{t:EShat}
\end{thm} % -----------------------------------------------------------------

\begin{rmk}
  Fig.~\ref{f:saddle}(c) shows the high accuracy of this
  asymptotic approximation, for all $\rho$ around unit size,
  apart from those very close to
  $\rho=1$, even at $\freq=30$, which is not huge.
  Around two digits of relative
  accuracy are achieved everywhere except $\rho\approx 1$, where $\hat\phi$
  is already exponentially small.
  The match includes the details of the exponentially small tail oscillations
  in $\rho>1$.
  (However, see Remark~\ref{r:decay} which discusses that for $\rho \ge \freq$,
  relative error grows.)
  \label{r:match}
\end{rmk}

\begin{proof}
In either case, the \FT\ to be estimated is
\be
\hat\phi(\rho\freq) = e^{-\freq} \int_{-1}^1
e^{\freq (\sqrt{1-z^2} + i\rho z)} dz
= e^{-\freq} \int_{-1}^1 e^{\freq p(z)} dz
~,\qquad\mbox{ where }\quad
p(z):=\sqrt{1-z^2} +i\rho z
~.
\label{ESFT}
\ee
We will apply saddle point integration in the
complex $z$ plane (see \cite[Thm.~7.1, p.~127]{olver};
note that we choose the opposite sign convention for $p(z)$).
This states that
$\int e^{\freq p(z)} dz = e^{\freq p(z_0)}\sqrt{\frac{2\pi}{-p''(z_0)\freq}}[1 + \bigO(\freq^{-1})]$,
where the saddle point $z_0$ is found by setting $p'=0$.
We must exhibit a smooth contour connecting $-1$
to $+1$, avoiding branch cuts,
passing through the saddle point(s), and along which $\re p(z)$
has its global maximum at $z_0$.
Note that the standard branch cut $(-\infty,0)$ for the square-root gives
cuts for $p(z)$ at $(-\infty,-1)$ and $(1,+\infty)$;
these cuts are shown in Fig.~\ref{f:saddle}(a).

{\bf Case (a).}
We take $0\le \rho<1$, since $\hat\phi$ has even symmetry.
Since $\re i\rho z$ becomes more negative as $\im z$ grows,
we choose the saddle $z_0 = i\rho/\sqrt{1-\rho^2}$ on the
positive imaginary axis; see Fig.~\ref{f:saddle}(a).
To show the existence of a valid contour we switch to standard elliptical
coordinates
\be
z = \cosh(u+iv)~,\qquad \re z = \cosh u \cos v~,\qquad \im z = \sinh u \sin v
~,
\label{ellip}
\ee
where $\mu\ge 0$ and $0\le v < 2\pi$ covers the plane.
Noting that $1-z^2 = -\sinh^2(u+iv)$, we get
\be
\re p(z) = \re p(u,v) = (\cosh u - \rho \sinh u) \sin v
~,
\label{repz}
\ee
which shows, remarkably, that the magnitude of the exponential
in \eqref{ESFT} is %a product $A(u)B(v)$
separable in this coordinate system.
% phase is a different product, not needed.
By solving $p'=0$ one finds that the saddle is at
$(u,v) = (\tanh^{-1} \rho, \pi/2)$,
where $\re p(z_0) = \sqrt{1-\rho^2}$.

For the second (right) half of the contour,
we need to show that there is some smooth open path in $(u,v)$ between
$(\tanh^{-1} \rho, \pi/2)$ and $(0,0)$ along which $\re p$ is
everywhere less than $\re p(z_0)$.
At the endpoint $p = 0$, which is indeed less than $\re p(z_0)$,
but it is possible that a barrier region of large $\re p$
prevents such a path existing.
However, no such barrier exists, as we now show.
The level curve $\re p(u,v) =\re p(z_0)$ has positive slope
in the $(u,v)$ plane (see Fig.~\ref{f:saddle}(a) inset).
This is clear since the level curve intersects each line $u=$ constant
only once in $(0,\pi/2)$, because $\sin v$ is monotonic there.
Furthermore, $\cosh u - \rho \sinh u$ is monotonically
decreasing in $[0,\tanh^{-1} \rho)$, as is apparent from its derivative,
so that the $v$-value of the intersection grows monotonically
with $u$.
This means that the ``bad'' region where $\re p(u,v) \ge \re p(z_0)$ is
confined to the upper left corner of the $(u,v)$ rectangle
(see inset), so there is no obstruction to crossing the diagonal
while remaining small.
The first half of the contour
may be chosen as a reflection of the second about the imaginary $z$ axis.
Thus a valid saddle contour exists.

At the saddle point, $p''(z_0) = -(1-\rho^2)^{3/2}$,
so that, taking the first term in the saddle point theorem
\cite[Thm.~7.1, p.~127]{olver}
gives \eqref{EShat1}.

{\bf Case (b).}
Now we consider $\rho>1$.
Solving $p'=0$ gives two saddle points on the real axis,
$z_0^{(\pm)} = \pm \rho/\sqrt{\rho^2-1}$, where $p(z_0^{(\pm)}) = 0$.
Since these lie on the standard branch cuts of the square-root,
to make use of saddle point integration connecting
$z=-1$ to $1$ in the upper half plane,
one must move the branch cuts downwards to expose more of the Riemann
sheet on which the contour lives.
In order to avoid regions of large integrand, the contour must
first head into the lower half-plane, pass up through $z_0^{(-)}$,
into the upper half plane, down through $z_0^{(+)}$,
and finish again from the lower half-plane; see Fig.~\ref{f:saddle}(b).
Since $p$ is the same at both saddles, they both contribute asymptotically.

To show the existence of a valid contour we examine \eqref{repz}.
The elliptical coordinates of the saddle points are $(\coth^{-1}\rho,0)$
and $(\coth^{-1}\rho,\pi)$.
The factor $\cosh u - \rho \sinh u$ vanishes on the ellipse
$u=\coth^{-1}\rho$ passing though the saddles, and,
since it is monotonically decreasing,
is positive for all smaller $u$ and negative for all larger $u$.
Thus $\re p<0$ everywhere in the upper half plane outside the ellipse
($u>\coth^{-1}\rho$, $0<v<\pi$), and in the lower half plane
inside the ellipse with the slit $[-1,1]$ omitted
($0<u<\coth^{-1}\rho$, $\pi<v<2\pi$).
Thus a smooth contour exists passing through these regions via the two saddles.

To apply the saddle point theorem one must isolate each saddle,
splitting the contour into left and right halves, and summing the two.
However on each half the theorem still cannot be used directly since
the start and end values $\re p(\pm1)=0$ are just as large
as at the saddles.
Thus we remove a fixed piece of the contour around $\pm1$,
allowing the theorem to be applied.
Using $p(z_0^{(\pm)}) = \pm i \sqrt{\rho^2-1}$
and $|p''(z_0^{(\pm)})| = (\rho^2-1)^{3/2}$ at the two saddles,
the steepest descent directions $e^{\mp i 3\pi/4}$,
and summing the two contributions gives \eqref{EShat2}.

Finally, we show that the contributions due to the fixed excluded
pieces of the contour touching $\pm1$ are of lower order.
Consider a contour in the lower half-plane
from $1$ to $1+b$, where $b\in\mathbb{C}$, $\im b<0$,
and $1+b$ is strictly inside the ellipse described above.
We have already explained that $p<0$ on this contour, apart from at $p(1)=0$.
Writing $z=1+t$, and using $p(1+t) = i\rho + \sqrt{-2t^2-t^2} + i\rho t$,
The contour integral is
$$ \int_0^b e^{\freq p(1+t)} dt = e^{i\rho}\int_0^b e^{\freq P(t)} dt
~,\qquad P(t) \sim -i\sqrt{2t}.
$$
All the conditions for
Laplace's method for contour integrals \cite[Thm.~6.1, p.125]{olver}
with power $\mu=1/2$ are satisfied, so
the contribution is $\sim -e^{i\rho}/\freq^2 = \bigO(\freq^{-2})$,
which is $\freq^{3/2}$ times smaller than the contribution from the saddles.
The same argument applies near $-1$.
Thus the end contributions can be ignored in \eqref{EShat2}.
\end{proof}

\begin{rmk}
  The theorem might be summarized informally as:
  ``the \FT\ of the exponential
  of a semicircle is asymptotically the exponential of a semicircle.''
  This becomes less mysterious when
    in Sec.~\ref{s:optim} we discuss that
    the ES kernel \eqref{ES} is close
    to the lowest prolate spheroidal wavefunction of degree zero (PSWF),
    and the latter is (up to scaling)
    equal to its own \FT\ restricted to $[-1,1]$ \cite{osipov}.
    Perhaps more
    surprising is the converse, that the PSWF is close to the exponential
    of a semicircle. %see Sec.~\ref{s:optim}.
%  Of course the set of functions that are, up to scaling,
%  precisely their own Fourier transforms is huge (since this transform
%  has only four eigenvalues $\pm\sqrt{2\pi}$ and $\pm i \sqrt{2\pi}$).
%
%  The Gaussian is, up to scaling, its own \FT,
%  which might be expressed as ``the \FT\ of the exponential
%  of a parabola is the exponential of a parabola.''
\end{rmk}  

The \FT\ decay suggested by \eqref{EShat2} is $\bigO(\rho^{-3/2})$,
ie $\bigO(\xi^{-3/2})$. % or when rescaled, $k^{-3/2}$.
However, we know that this cannot be true,
because the kernel has discontinuities at $z=\pm 1$ of strength $e^{-\freq}$,
and is otherwise smooth, so
the correct asymptotic should be (at fixed $\freq$),
$\phi\hat(\xi) \sim 2 e^{-\freq} \frac{\sin \xi}{\xi} = \bigO(\xi^{-1})$.
What went wrong with \eqref{EShat2}?
The problem is that the result cannot be applied at fixed $\freq$
and the limit $\rho\to\infty$: the implied constant in the error
term is unknown, and in fact cannot be uniformly bounded in $\rho$.
These growing errors are due to the saddles $z_0^{(\pm)}$ approaching
the endpoints $\pm 1$ where the square-root singularities are.

This motivates the following rigorous statement which applies uniformly
as $\rho\to\infty$ but has the $\beta$-dependence of the error
explicit.

\begin{thm} % ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
  Fix the shape parameter $\freq>0$. Then,
  \be
  \hat\phi(\rho\beta) = 2e^{-\beta}\frac{\sin \xi}{\xi}
  \left[ 1 + \sqrt{\freq}\bigO(\rho^{-1/2}) \right]
  ~,\qquad \rho\to\infty
  \label{EShat3}
  \ee
  
  
\label{t:EShattail}
\end{thm}







\subsection{NUFFT error bounds for the ES kernel}

The goal is now 



\begin{rmk}
Since the algebraic decay in \eqref{EShat2} appears to be
$\rho^{-3/2}$ (ie $\xi^{-3/2}$), one is tempted to
directly insert this into the sum in \eqref{epsest},
and declare victory.
However, this is incorrect, since the implied constant in the error
term in \eqref{EShat2} is unknown, and is in fact not uniformly bounded
in $\rho$.
Since the kernel has discontinuities at $z=\pm 1$ of strength $e^{-\freq}$,
the correct asymptotic is (at fixed $\freq$),
$\phi\hat(\xi) \sim 2 e^{-\freq} \frac{\sin \xi}{\xi} = \bigO(\xi^{-1})$.
Thus relative errors in \eqref{EShat2} diverge beyond the transition
region of $\rho \approx \freq$.
\label{r:decay}
\end{rmk}







scaled to width.

$w=1,2,\ldots$
sets the support of the kernel in units of the uniform grid point spacing, and









\begin{rmk}[KB error bounds]
  \label{fourmont}
  In the \KB\ case, rigorous estimates of \eqref{gkx}
  which take cancellation due to changing phases into account
  are due to Fourmont, and are quite technical \cite[p.~30-38]{fourmontthesis}.
  The results are summarized in \cite[Sec.~4]{fourmont}, and used in
  \cite[p.~30-31]{pottshabil} to give the estimate in \cite[App.~C]{nfft}.
  *** GIVE FORMULA TO COMPARE.
\end{rmk}

*** STATE ES AND KB SIMILAR BNDS.




% wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww
\subsection{Choice of spreading width $w$}
\label{s:w}

explain why base 10 (one digit per node width).
comes from $e^{-\gamma}$.

and explain $\beta = 2.3 w$.


\subsection{Quadrature evaluation of the kernel Fourier transform}
\label{s:p}

justify  $p\ge 2+1.5 w$ gives sufficient accuracy
over the $k$ range,
%apparent exponential convergence rate

This quadrature scheme must compute $\hat\psi$
with relative accuracy
somewhat smaller than $\eps$, the requested precision,
because \eqref{pk1} involves dividing by $\hat\psi$ to amplify
high frequencies.
The extra factor is $\rmax$, the ratio between the smallest and largest
amplification factors. Using $n\approx \rat N$ and
\eqref{EShat1} and $\beta \approx \pi w( 1- 1/2\rat)$,
this is
\be
\rmax := \frac{\hat\psi(0)}{\hat\psi(N/2)}
= \frac{\hat\phi(0)}{\hat\phi(\pi w/2\rat)}
\approx e^{\freq - \sqrt{\freq^2 - (\pi w/2\rat)^2}}
= e^{\bigl(1 - \sqrt{1-(2\rat-1)^{-2}}\bigr)\freq}
\label{rmax}
\ee
which for $\rat=2$ gives $\rmax \approx e^{0.057\,\freq}$.
Thus, since $\beta\le40$ for any $\eps$
up to double precision accuracy, $\rmax\le 10$.
A full error analysis of the convergence of Gauss--Legendre quadrature
for the Fourier integral would be involved, since $\phi$ in \eqref{ES}
formally has square-root singularities at $\pm 1$.
However, these singularities have a strength that dies at the same
exponential rate with $\freq$
as the aliasing errors discussed in this sec, so have
little consequence.
In practice we find that $p\ge 2+1.5 w$ gives sufficient accuracy
over the $k$ range,
%apparent exponential convergence rate 
meaning that the maximum quadrature spacing is around one
grid spacing $h$.






\subsection{Discussion of optimality of the spreading kernel}
\label{s:optim}

One might ask what kernel function in $[-1,1]$ has optimal Fourier localization
to the frequency band $[-\freq,\freq]$,
ie *** in sense of error bound.
Give L2 error bnd version.
In the $L^2$ norm this is the prolate spheroidal wave function (PSWF)
of order and degree zero, $\psi_0$,
with Slepian frequency parameter $c=\freq$
\cite{osipov}. REF Slepian.
Since $\phi_0$ is a normalized eigenfunction of the projection operator
($Q_c$ in \cite{osipov}) onto frequencies
of magnitude less than $\freq$,
its eigenvalue $\mu_0\le 1$ gives the
squared mass
$2\pi \int_{|\xi|<\freq}|\hat\psi_0(\xi)|^2 d\xi$,
and the remaining mass is
$1-\mu_0 =2\pi \int_{|\xi|>\freq}|\hat\psi_0(\xi)|^2 d\xi$.
It was proven by Fuchs \cite{fuchs} that
\be
1 - \mu_0 \sim 4\sqrt{\pi\freq} e^{-2\freq} ~, \qquad \freq\to\infty~,
\label{fuchs}
\ee
which shows that the $L_2$-norm of $\hat\psi_0$ outside
$[-\freq,\freq]$ is exponentially small with rate $e^{-\freq}$.
This rate is exactly the same as
proven by Fourmont and Potts for the \KB, and in Thm ** for ES.
This is highly suggestive that $e^{-\freq}$ is the fastest
rate of decay.
However, since the $L^2$ norm does not bound the $L^1$ norm
nor the sum appearing in error estimate,
it is hard to make a rigorous connection.

Having motivated the PSWF $\psi_0(z)$ as a close-to-optimal
spreading function, one is led to ask:
how this $\psi_0$ related to either the
ES kernel \eqref{ES} or the \KB\ kernel \eqref{KB},
in the limit $\freq\to\infty$ ?
Which is closer, and what are rigorous bounds on how close?
We can only give incomplete answers at this stage.
One might hope that
standard WKBJ approximation applied to the PSWF defining
ODE $-((1-z^2)\psi')'+\freq^2z^2\psi = \chi_0\psi$
would reveal a simple form similar to the ES or KB kernels,
but this is not so.
However, other known asymptotics offer clues:
Slepian \cite[(1.4)]{slepian65}
showed that $\psi_0(z) = C e^{\freq\sqrt{1-z^2}} (1-z^2)^{-1/4}
(1+\sqrt{1-z^2})^{-1/2}(1 + \bigO(\freq^{-1}))$
in $\freq^{-1/2} \le |z| \le 1-\freq^{-1}$,
and
that $\psi_0(z) = C I_o(\freq \sqrt{1-z^2}) (1 + O(\freq^{-1}))$
for $1-\freq^{-1} \le |z| \le 1$.
We note that, apart from the factor $(1+\sqrt{1-z^2})^{-1/2}$
which varies by a factor of only $\sqrt{2}$,
this matches the KB kernel for all $|z|\ge \freq^{-1/2}$.
Inside the central (turning point) region
$|z| = \bigO(\freq^{-1/2})$,
the bump tends to a Gaussian
$\psi_0(z) = C e^{-\beta z^2 /2} + \bigO(\freq^{-1})$,
which has a width differing by only $4\%$ from that of the
optimal truncated Gaussian shown in Fig.~\ref{f:kernel}.
This last form comes from expansion in terms of Hermite functions
\cite[\S 3.25]{meixner} % confusing since call's them D
%\cite{fuchs}
\cite[Sec.~8.6]{osipov}.
%Slepian has asymptotics in terms of Weber functions. (also called D !)
Recent results by Ogilvie \cite[Sec.~4.4]{ogilvie}
and Dunster \cite{dunster}
give asymptotic expressions over larger domains
in terms of parabolic cylinder functions;
the former are uniform over $|z|\le 1-\eps$ and the
latter $\freq^{-1/2}\le|z|\le1$.
However, due to their complexity,
it is not clear that they can give
any clearer connection to the ES and KB kernels than presented above.


Note that the bounds of Fourmont require removing the abs vals
from the sinc function of \eqref{KBhat}, allowing subtle cancellations;
the function is not itself in $L^1$.

Fessler wasn't able to beat KB much by numerical optimization.
We have performed simliar experiments on the
exponential of polynomials, and also are unable to beat the \KB\ or
ES kernel.

Very recently, $c\to\infty$ asymptotics of PSWF were derived

*** todo.


\section{Implementation issues}

spreading cost dominated by random access.
sorting of \NU\ pts.

\subsection{OpenMP parallelization}

Spreading:

Type 1 spreading:
block the output array by thread.

Type 2 interpolation is much simpler to parallelize:
thread over target points.

FFTW is multi-threaded.

Type-3:
computation of $p_k$ is expensive due to $pN$ cosines,
omp it.

Many other loops that have only a couple of flops
per element are RAM access limited and do not benefit
from parallelization.


\section{Performance tests}

First demo the kernel eval speed.

Focus on 3d - do rand cube, do sphere (high density around origin).



relative error vs $\eps$

time vs problem size.
Compare to plain FFT.

\begin{rmk}
  One way to express the cost of the type-1 NUFFT
  is to compare it to the cost of the FFT in the case of $M=N$
  and uniform source points.
  Purely from step 2, ignoring the log factors,
  this cost must be at least a factor $\rat^d$; recall we choose $\rat=2$.
  In fact the spreading dominates,
  *** say more.
\end{rmk}
\begin{rmk}
  The cost for the type-2 NUFFT is similiar to that of type-1.
  We will see that it parallelizes slightly more efficiently
  than type-1.
\end{rmk}
\begin{rmk}
  The cost for the FFTs in the type-3 NUFFT is $\rat^{2d}$ times that of
  the FFT, for uniform inputs and outputs.
\end{rmk}




strong omp scaling.

comparision vs NFFT.


% ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc
\section{Conclusion}



GPU version; however tricky for spreading to \U\ grid, needed in types 1 and 3.


Remains open to prove a relation between the zero prolate spheroidal
wavefunction and the ES kernel \eqref{ES}.


\section*{Acknowledgments}

We are grateful for helpful discussions with Leslie Greengard,
Charlie Epstein, Marina Spivak, Andras Pataki, Arthur Migdal,
Mark Dunster,
and Daniel Potts.


% BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
\bibliographystyle{abbrv}
\bibliography{alex}
\end{document}
